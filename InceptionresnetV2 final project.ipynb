{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnonyBOSS/detect-AI/blob/main/InceptionresnetV2%20final%20project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMyqpwGcXbLS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Function to download dataset\n",
        "def download_dataset(dataset_name):\n",
        "    path = kagglehub.dataset_download(dataset_name)\n",
        "    print(f\"Downloaded {dataset_name} to {path}\")\n",
        "    return path\n",
        "\n",
        "# Download datasets\n",
        "main_dataset_path = download_dataset(\"alessandrasala79/ai-vs-human-generated-dataset\")\n",
        "cifake_path = download_dataset(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "\n",
        "# Function to load dataset from CSV\n",
        "def load_csv_dataset(base_dir, train_csv, test_csv):\n",
        "    df_train = pd.read_csv(os.path.join(base_dir, train_csv))\n",
        "    df_test = pd.read_csv(os.path.join(base_dir, test_csv))\n",
        "\n",
        "    print(\"Train CSV Columns:\", df_train.columns)\n",
        "    print(\"Test CSV Columns:\", df_test.columns)\n",
        "\n",
        "    # Determine correct column names\n",
        "    file_col_train = 'file_name' if 'file_name' in df_train.columns else df_train.columns[1]  # Default to second column\n",
        "    file_col_test = 'file_name' if 'file_name' in df_test.columns else ('id' if 'id' in df_test.columns else df_test.columns[0])\n",
        "    label_col_train = 'label' if 'label' in df_train.columns else df_train.columns[-1]\n",
        "    label_col_test = 'label' if 'label' in df_test.columns else df_test.columns[-1]\n",
        "\n",
        "    df_train[file_col_train] = df_train[file_col_train].apply(lambda x: os.path.join(base_dir, x))\n",
        "    df_test[file_col_test] = df_test[file_col_test].apply(lambda x: os.path.join(base_dir, x))\n",
        "\n",
        "    return df_train[[file_col_train, label_col_train]], df_test[[file_col_test, label_col_test]]\n",
        "\n",
        "# Load main dataset\n",
        "df_train, df_test = load_csv_dataset(main_dataset_path, 'train.csv', 'test.csv')\n",
        "\n",
        "# Function to load images from folder structure\n",
        "def load_folder_dataset(base_dir, train_folder, test_folder):\n",
        "    train_real = [os.path.join(base_dir, train_folder, \"REAL\", img) for img in os.listdir(os.path.join(base_dir, train_folder, \"REAL\"))]\n",
        "    train_fake = [os.path.join(base_dir, train_folder, \"FAKE\", img) for img in os.listdir(os.path.join(base_dir, train_folder, \"FAKE\"))]\n",
        "    test_real = [os.path.join(base_dir, test_folder, \"REAL\", img) for img in os.listdir(os.path.join(base_dir, test_folder, \"REAL\"))]\n",
        "    test_fake = [os.path.join(base_dir, test_folder, \"FALE\", img) for img in os.listdir(os.path.join(base_dir, test_folder, \"FAKE\"))]\n",
        "\n",
        "    train_images = train_real + train_fake\n",
        "    train_labels = [1] * len(train_real) + [0] * len(train_fake)\n",
        "    test_images = test_real + test_fake\n",
        "    test_labels = [1] * len(test_real) + [0] * len(test_fake)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "# Load CIFAKE dataset\n",
        "cifake_train_paths, cifake_train_labels, cifake_test_paths, cifake_test_labels = load_folder_dataset(cifake_path, 'train', 'test')\n",
        "\n",
        "# Merge datasets\n",
        "train_paths = list(df_train.iloc[:, 0]) + cifake_train_paths\n",
        "train_labels = list(df_train.iloc[:, 1]) + cifake_train_labels\n",
        "val_paths = list(df_test.iloc[:, 0]) + cifake_test_paths\n",
        "val_labels = list(df_test.iloc[:, 1]) + cifake_test_labels\n",
        "\n",
        "# Split dataset\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    train_paths, train_labels, test_size=0.1, stratify=train_labels, random_state=42)\n",
        "\n",
        "print(f\"Total Training Images: {len(train_paths)}\")\n",
        "print(f\"Total Validation Images: {len(val_paths)}\")\n",
        "\n",
        "# Image preprocessing functions\n",
        "def preprocess_image(image_path, label, is_training=True):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    if is_training:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "    return image, label\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(lambda x, y: preprocess_image(x, y, True), num_parallel_calls=AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(lambda x, y: preprocess_image(x, y, False), num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"Dataset is ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVq8AmkeX1QJ",
        "outputId": "8eb58c9a-421d-4d23-eb0e-08fc454646b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded alessandrasala79/ai-vs-human-generated-dataset to /root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4\n",
            "Downloaded birdy654/cifake-real-and-ai-generated-synthetic-images to /root/.cache/kagglehub/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/versions/3\n",
            "Train CSV Columns: Index(['Unnamed: 0', 'file_name', 'label'], dtype='object')\n",
            "Test CSV Columns: Index(['id'], dtype='object')\n",
            "Total Training Images: 161955\n",
            "Total Validation Images: 17995\n",
            "Dataset is ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D"
      ],
      "metadata": {
        "id": "gf-3N0h-X7D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model (optional)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the full model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),  # Converts feature maps to a vector\n",
        "    Dense(128, activation='relu'),  # Fully connected layer\n",
        "    Dropout(0.5),  # Prevent overfitting\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "fvQmPLxDX-Zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cd19b1-1c04-4681-902c-8bab8d0d68c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D, BatchNormalization"
      ],
      "metadata": {
        "id": "h9e1fU8Z9QBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Ue-1NiTn9QOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "        BinaryAccuracy()\n",
        "    ]\n",
        ")\n",
        "\n",
        "call_backs = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-6\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir='logs',\n",
        "        histogram_freq=1\n",
        "    ),\n",
        "    tf.keras.callbacks.TerminateOnNaN(),\n",
        "    tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 0.9 ** epoch),\n",
        "    tf.keras.callbacks.CSVLogger('training.log'),\n",
        "    tf.keras.callbacks.History(),\n",
        "    tf.keras.callbacks.LambdaCallback(\n",
        "        on_epoch_begin=lambda epoch, logs: print(f\"Epoch {epoch} started\")\n",
        "    )\n",
        "]\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "k9CRDK_LYArg",
        "outputId": "a5c34c63-8cb8-4422-d042-57a8cfcd45bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inception_resnet_v2 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1536\u001b[0m)          │      \u001b[38;5;34m54,336,736\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m196,736\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inception_resnet_v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,533,601\u001b[0m (208.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,533,601</span> (208.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m196,865\u001b[0m (769.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">196,865</span> (769.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m54,336,736\u001b[0m (207.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> (207.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl8zcKi-YCQc",
        "outputId": "570a9ed2-96ee-4ae0-9ba1-c6471215c9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m5062/5062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m960s\u001b[0m 182ms/step - binary_accuracy: 0.7102 - loss: 0.5529 - val_binary_accuracy: 0.8331 - val_loss: 0.3759\n",
            "Epoch 2/5\n",
            "\u001b[1m5062/5062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 166ms/step - binary_accuracy: 0.8245 - loss: 0.3894 - val_binary_accuracy: 0.8554 - val_loss: 0.3319\n",
            "Epoch 3/5\n",
            "\u001b[1m5062/5062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m839s\u001b[0m 165ms/step - binary_accuracy: 0.8441 - loss: 0.3530 - val_binary_accuracy: 0.8628 - val_loss: 0.3147\n",
            "Epoch 4/5\n",
            "\u001b[1m5062/5062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 165ms/step - binary_accuracy: 0.8560 - loss: 0.3313 - val_binary_accuracy: 0.8655 - val_loss: 0.3021\n",
            "Epoch 5/5\n",
            "\u001b[1m5062/5062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 165ms/step - binary_accuracy: 0.8635 - loss: 0.3162 - val_binary_accuracy: 0.8744 - val_loss: 0.2878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"alessandrasala79/ai-vs-human-generated-dataset\")\n",
        "base_dir = path\n",
        "print(\"Path to dataset files:\", path)\n",
        "def preprocess_val(image):\n",
        "    image = tf.image.resize(image, [224, 224], method=tf.image.ResizeMethod.BICUBIC)  # Resize\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    mean = tf.constant([0.485, 0.456, 0.406])\n",
        "    std = tf.constant([0.229, 0.224, 0.225])\n",
        "    image = (image - mean) / std  # Normalize\n",
        "\n",
        "    return image\n",
        "\n",
        "def load_test_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = preprocess_val(image)\n",
        "    return image\n",
        "train_csv_path = os.path.join(base_dir, 'train.csv')\n",
        "test_csv_path  = os.path.join(base_dir, 'test.csv')\n",
        "\n",
        "# Reading the training CSV file\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "# Example of a row: file_name=\"train_data/041be3153810...\", label=0 or 1\n",
        "\n",
        "# Reading the testing CSV file\n",
        "df_test = pd.read_csv(os.path.join(base_dir, 'test.csv'))\n",
        "# Exemple: df_test['id'] = \"test_data/e25323c62af644fba97afb846261b05b.jpg\", etc.\n",
        "\n",
        "# Adding the full path to the file_name instead of just \"trainORtest_data/xxx.jpg\"\n",
        "df_test['id'] = df_test['id'].apply(lambda x: os.path.join(base_dir, x))\n",
        "df_train['file_name'] = df_train['file_name'].apply(lambda x: os.path.join(base_dir, x))\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(df_test['id'].values)\n",
        "test_ds = test_ds.map(load_test_image).batch(batch_size)\n",
        "testf_csv_path  = os.path.join(base_dir, 'test.csv')\n",
        "dff_test = pd.read_csv(os.path.join(base_dir, 'test.csv'))\n",
        "y_test_preds = model.predict(test_ds).flatten()\n",
        "y_test_preds = (y_test_preds > 0.5).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': dff_test['id'],\n",
        "    'label': y_test_preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Submission file saved!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzMmcbesXzLz",
        "outputId": "6aaedf34-1d23-4465-f5b5-4794f2442308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 719ms/step\n",
            "Submission file saved!\n"
          ]
        }
      ]
    }
  ]
}